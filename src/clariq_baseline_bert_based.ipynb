{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Aj5cB8x9R7Lp"
      },
      "source": [
        "## Installing transformer-rankers and dependencies\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ndWAB7rQRg0J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Guzpenha/transformer_rankers.git\n",
            "  Cloning https://github.com/Guzpenha/transformer_rankers.git to c:\\users\\grego\\appdata\\local\\temp\\pip-req-build-qy2kb3gg\n",
            "  Resolved https://github.com/Guzpenha/transformer_rankers.git to commit 637999ede0c1ae4389ccef5ac09337a5a2674ed7\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/Guzpenha/transformer_rankers.git 'C:\\Users\\grego\\AppData\\Local\\Temp\\pip-req-build-qy2kb3gg'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ipython\n",
            "ir-datasets\n",
            "numpy==1.22.0\n",
            "pandas==1.0.4\n",
            "pytrec-eval==0.4\n",
            "sacred==0.8.1\n",
            "torch==1.13.1\n",
            "tqdm==4.46.1\n",
            "transformers==3.0.2\n",
            "scipy==1.4.1\n",
            "sentence-transformers==2.0.0\n",
            "faiss-cpu==1.6.3\n",
            "dataclasses\n",
            "pyserini==0.13.0\n",
            "wget==3.2\n",
            "py7zr==0.20.2\n",
            "wandb==0.9.7\n",
            "python-terrier==0.7.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100   272  100   272    0     0   1049      0 --:--:-- --:--:-- --:--:--  1062\n",
            "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Guzpenha/transformer_rankers.git\n",
        "!curl https://raw.githubusercontent.com/Guzpenha/transformer_rankers/master/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "G1AfQvDVjLJU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'mvn' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n",
            "'ls' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "#Install Anserini which is also a requirement for part of transformer-rankers (BM25 Negative Samplers)\n",
        "#!pip install maven -qq\n",
        "#!git clone --recurse-submodules https://github.com/castorini/anserini.git\n",
        "#!cd anserini; \n",
        "!mvn clean package appassembler:assemble -DskipTests -Dmaven.javadoc.skip=true\n",
        "!ls anserini/target/appassembler/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5Oa0JWLcSAeF"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Google colab with torch 1.5 doesnt see the GPU\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install -I torch==1.4.0\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#This should ouptut a GPU device name\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\cuda\\__init__.py:419\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    408\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the name of a device.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \n\u001b[0;32m    410\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mname\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\cuda\\__init__.py:449\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    440\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Gets the properties of a device.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
            "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "#Google colab with torch 1.5 doesnt see the GPU\n",
        "#!pip install -I torch==1.4.0\n",
        "import torch\n",
        "torch.cuda.get_device_name(0)  #This should ouptut a GPU device name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lG57GzJWSOPY"
      },
      "source": [
        "## Downloading ClariQ data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jv2XbhwqSP7O"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "La syntaxe de la commande n'est pas correcte.\n",
            "La syntaxe du nom de fichier, de r�pertoire ou de volume est incorrecte.\n",
            "La syntaxe du nom de fichier, de r�pertoire ou de volume est incorrecte.\n",
            "La syntaxe du nom de fichier, de r�pertoire ou de volume est incorrecte.\n",
            "'mv' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!mkdir data/clariq\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/dev.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/train.tsv\n",
        "!cd data/clariq; wget https://github.com/aliannejadi/ClariQ/raw/master/data/question_bank.tsv\n",
        "!mv data/clariq/train.tsv data/clariq/train_original.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "isQIZq0XSljz"
      },
      "source": [
        "## Preprocess ClariQ for transformer-rankers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ylAtrf1lSoP-"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/train_original.tsv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_original.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m train \u001b[38;5;241m=\u001b[39m train[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_request\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train_original.tsv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data_path = \"./data/\"\n",
        "\n",
        "train = pd.read_csv(data_path+\"train_original.tsv\", sep=\"\\t\")\n",
        "valid = pd.read_csv(data_path+\"dev.tsv\", sep=\"\\t\")\n",
        "\n",
        "train = train[[\"initial_request\", \"question\"]]\n",
        "train.columns = [\"query\", \"clarifying_question\"]\n",
        "train = train[~train[\"clarifying_question\"].isnull()]\n",
        "\n",
        "valid = valid[[\"initial_request\", \"question\"]]\n",
        "valid.columns = [\"query\", \"clarifying_question\"]\n",
        "valid = valid[~valid[\"clarifying_question\"].isnull()]\n",
        "\n",
        "train.to_csv(data_path+\"train.tsv\", sep=\"\\t\", index=False)\n",
        "valid.to_csv(data_path+\"valid.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "zAEKgEf6TNzz",
        "outputId": "b3ba1a81-d269-4803-84a7-eb995246bcdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>clarifying_question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you interested in seeing barack obamas family</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know barack obamas geneology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know about obamas ancestors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>would you like to know who is currently alive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tell me about Obama family tree.</td>\n",
              "      <td>are you looking for biological information on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              query                                clarifying_question\n",
              "0  Tell me about Obama family tree.  are you interested in seeing barack obamas family\n",
              "1  Tell me about Obama family tree.     would you like to know barack obamas geneology\n",
              "2  Tell me about Obama family tree.      would you like to know about obamas ancestors\n",
              "3  Tell me about Obama family tree.  would you like to know who is currently alive ...\n",
              "4  Tell me about Obama family tree.  are you looking for biological information on ..."
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For transformer-rankers we only need a pandas DF with query (here the initial request) \n",
        "# and relevant documents (here the clarifying questions).\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "iOeCaSHOVDLK",
        "outputId": "bc93f129-719b-4ede-fe17-4ae59cf2e89a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Q00001</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Q00002</td>\n",
              "      <td>a total cholesterol of 180 to 200 mgdl 10 to 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Q00003</td>\n",
              "      <td>about how many years experience do you want th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Q00004</td>\n",
              "      <td>according to anima the bible or what other source</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Q00005</td>\n",
              "      <td>ae you looking for examples of septic system d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  question_id                                           question\n",
              "0      Q00001                                                NaN\n",
              "1      Q00002  a total cholesterol of 180 to 200 mgdl 10 to 1...\n",
              "2      Q00003  about how many years experience do you want th...\n",
              "3      Q00004  according to anima the bible or what other source\n",
              "4      Q00005  ae you looking for examples of septic system d..."
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will sample negative samples for training using the question bank\n",
        "question_bank = pd.read_csv(data_path+\"clariq/question_bank.tsv\", sep=\"\\t\")\n",
        "question_bank.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bAILVs8MUL9k"
      },
      "source": [
        "## Training a transformer-ranker for ClariQ (RQ2)\n",
        "\n",
        "The problem is to retrieve the most relevant clarifying question for a given query. We will train a BERT-ranker using transformer-rankers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1d48517fab1f4826a99b5053f71ccf5c",
            "cd8edeb69bca4a6aa6aa644cd3c896d6",
            "e66069b612964f37b897276d35fb341b",
            "5e0c3ce7cd944b92afc1ec5fd5c21773",
            "9f88dcb06d364fcb9be11a7980b10775",
            "9d07b3f2a9094d64867f893667c253bf",
            "0a3f5ed58d9d4d308643184b11cef78a",
            "2996a43d436a4d7aae36c7d44626af4a",
            "921b8a33cba14558b84c9c1916e47e50",
            "5690414a8bd544aca30f2fd97a3cf905",
            "979b0281fd4e49df865b0c184e8d1bb6",
            "f21f758f54e8481c973853989e87d200",
            "ade0cde134a84b63bedde4facd44f428",
            "251e3d3acb5b49e7ba596286f5b367ab",
            "db8f058e9381429686f007982fcc577d",
            "a05fb8ec4b844c69b0d8adc1b659168f",
            "bc067504beca4713b3a5796551a99414",
            "02932572abae45d0b615af4c68af473e",
            "94d38c0feeff4b768ab3188ec4440f35",
            "a7653c51b7b44cb783278ee3fd4f5d59",
            "c1fe00e2fd6f4d36b844ec1bdc39d6a1",
            "131daad9d4bb457c84510b5326ed4cf5",
            "e289eaabf4964d50824ed92df65b9744",
            "e5ed1b54d7ef464c831572fc2ef22523"
          ]
        },
        "colab_type": "code",
        "id": "EC9XL8YvTXoE",
        "outputId": "f42190e1-5b2b-4894-aa05-bd077c3a3d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:23,222 [INFO] Lock 139694630808936 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2020-08-07 17:03:23,224 [INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpc1wij842\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d48517fab1f4826a99b5053f71ccf5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:03:23,520 [INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,522 [INFO] creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,527 [INFO] Lock 139694630808936 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "2020-08-07 17:03:23,529 [INFO] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "2020-08-07 17:03:23,577 [INFO] Train instances per batch 12\n",
            "2020-08-07 17:03:23,638 [INFO] Generating instances with signature set_train_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 187/187 [00:01<00:00, 97.69it/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:25,562 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:33,169 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:33,383 [INFO] Set train Instance 0 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,384 [INFO] Set train Instance 0 document \n",
            "\n",
            "are you interested in indiana child support\n",
            "\n",
            "2020-08-07 17:03:33,385 [INFO] Set train Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2024, 2017, 4699, 1999, 5242, 2775, 2490, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,388 [INFO] Set train Instance 1 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,389 [INFO] Set train Instance 1 document \n",
            "\n",
            "which counties law would you like clarification on\n",
            "\n",
            "2020-08-07 17:03:33,391 [INFO] Set train Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2029, 5721, 2375, 2052, 2017, 2066, 18856, 8486, 10803, 2006, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,393 [INFO] Set train Instance 2 query \n",
            "\n",
            "Child support in Indiana?[...]\n",
            "\n",
            "2020-08-07 17:03:33,394 [INFO] Set train Instance 2 document \n",
            "\n",
            "would you like a list of forms for processing child support claims\n",
            "\n",
            "2020-08-07 17:03:33,396 [INFO] Set train Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2775, 2490, 1999, 5242, 1029, 102, 2052, 2017, 2066, 1037, 2862, 1997, 3596, 2005, 6364, 2775, 2490, 4447, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:33,516 [INFO] Total of 16981 instances were cached.\n",
            "2020-08-07 17:03:33,538 [INFO] Generating instances with signature set_val_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 150.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:33,881 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:35,882 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:35,915 [INFO] Set val Instance 0 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,916 [INFO] Set val Instance 0 document \n",
            "\n",
            "are you interested in brooks brothers clearance shirts\n",
            "\n",
            "2020-08-07 17:03:35,919 [INFO] Set val Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2024, 2017, 4699, 1999, 8379, 3428, 14860, 11344, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,922 [INFO] Set val Instance 1 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,924 [INFO] Set val Instance 1 document \n",
            "\n",
            "do you want to know about brooks brothers clearance center in garland nc\n",
            "\n",
            "2020-08-07 17:03:35,926 [INFO] Set val Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 2113, 2055, 8379, 3428, 14860, 2415, 1999, 17017, 13316, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,927 [INFO] Set val Instance 2 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:35,929 [INFO] Set val Instance 2 document \n",
            "\n",
            "do you want to shop online for clothes\n",
            "\n",
            "2020-08-07 17:03:35,930 [INFO] Set val Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 4497, 3784, 2005, 4253, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:35,961 [INFO] Total of 4411 instances were cached.\n",
            "2020-08-07 17:03:35,982 [INFO] Generating instances with signature set_test_n_cand_docs_45_ns_sampler_BM25NS_seq_max_l_50_sample_-1_for_classification_using_BertTokenizer\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:00<00:00, 142.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:36,342 [INFO] Encoding examples using tokenizer.batch_encode_plus().\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:03:38,301 [INFO] Transforming examples to instances format.\n",
            "2020-08-07 17:03:38,479 [INFO] Set test Instance 0 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,480 [INFO] Set test Instance 0 document \n",
            "\n",
            "are you interested in brooks brothers clearance shirts\n",
            "\n",
            "2020-08-07 17:03:38,483 [INFO] Set test Instance 0 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2024, 2017, 4699, 1999, 8379, 3428, 14860, 11344, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,485 [INFO] Set test Instance 1 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,487 [INFO] Set test Instance 1 document \n",
            "\n",
            "do you want to know about brooks brothers clearance center in garland nc\n",
            "\n",
            "2020-08-07 17:03:38,488 [INFO] Set test Instance 1 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 2113, 2055, 8379, 3428, 14860, 2415, 1999, 17017, 13316, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,490 [INFO] Set test Instance 2 query \n",
            "\n",
            "Find Brooks Brothers clearance.[...]\n",
            "\n",
            "2020-08-07 17:03:38,491 [INFO] Set test Instance 2 document \n",
            "\n",
            "do you want to shop online for clothes\n",
            "\n",
            "2020-08-07 17:03:38,493 [INFO] Set test Instance 2 features \n",
            "\n",
            "InputFeatures(input_ids=[101, 2424, 8379, 3428, 14860, 1012, 102, 2079, 2017, 2215, 2000, 4497, 3784, 2005, 4253, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
            "\n",
            "2020-08-07 17:03:38,530 [INFO] Total of 4411 instances were cached.\n",
            "2020-08-07 17:03:38,776 [INFO] Lock 139694630845128 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "2020-08-07 17:03:38,780 [INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmptkm5vfhr\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "921b8a33cba14558b84c9c1916e47e50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:03:38,986 [INFO] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,988 [INFO] creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,990 [INFO] Lock 139694630845128 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "2020-08-07 17:03:38,992 [INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "2020-08-07 17:03:38,994 [INFO] Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "2020-08-07 17:03:39,217 [INFO] Lock 139694572620656 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "2020-08-07 17:03:39,219 [INFO] https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmplnt80y73\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc067504beca4713b3a5796551a99414",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2020-08-07 17:04:21,308 [INFO] storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:21,310 [INFO] creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:21,312 [INFO] Lock 139694572620656 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "2020-08-07 17:04:21,313 [INFO] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "2020-08-07 17:04:24,718 [INFO] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "2020-08-07 17:04:24,719 [INFO] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-08-07 17:04:24,727 [INFO] Device cuda\n",
            "2020-08-07 17:04:24,728 [INFO] Num GPU 1\n",
            "2020-08-07 17:04:29,615 [INFO] Total batches per epoch : 1416\n",
            "2020-08-07 17:04:29,616 [INFO] Validating every 1 epoch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 1416/1416 [07:12<00:00,  3.28it/s]\n",
            "100%|██████████| 368/368 [00:27<00:00, 13.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-08-07 17:12:09,138 [INFO] Epoch 1 val nDCG@10 0.951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformer_rankers.trainers import transformer_trainer\n",
        "from transformer_rankers.datasets import dataset\n",
        "from transformer_rankers.negative_samplers import negative_sampling\n",
        "from transformer_rankers.eval import results_analyses_tools\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "logging.basicConfig(\n",
        "  level=logging.INFO,\n",
        "  format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "  handlers=[\n",
        "      logging.StreamHandler(sys.stdout)\n",
        "  ]\n",
        ")\n",
        "\n",
        "#The combination of query and question are not that big.\n",
        "max_seq_len = 50\n",
        "\n",
        "#Lets use an almost balanced amount of positive and negative samples during training.\n",
        "average_relevant_per_query = train.groupby(\"query\").count().mean().values[0]\n",
        "\n",
        "#Instantiate BM25 negative sampler.\n",
        "ns_train = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query) , \n",
        "                    \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "ns_val = negative_sampling.BM25NegativeSamplerPyserini(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query), \n",
        "                    \"/content/data/clariq/anserini_train/\", -1, \"./anserini/\")\n",
        "\n",
        "# We could also use random sampling which does not require Anserini.\n",
        "# ns_train = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "# ns_val = negative_sampling.RandomNegativeSampler(list(question_bank[\"question\"].values[1:]), int(average_relevant_per_query))\n",
        "\n",
        "#Create the loaders for the dataset, with the respective negative samplers\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "dataloader = dataset.QueryDocumentDataLoader(train_df=train,\n",
        "                    val_df=valid, test_df=valid,\n",
        "                    tokenizer=tokenizer, negative_sampler_train=ns_train,\n",
        "                    negative_sampler_val=ns_val, task_type='classification',\n",
        "                    train_batch_size=12, val_batch_size=12, max_seq_len=max_seq_len,\n",
        "                    sample_data=-1, cache_path=\"./data/clariq/\")\n",
        "\n",
        "train_loader, val_loader, test_loader = dataloader.\\\n",
        "  get_pytorch_dataloaders()\n",
        "\n",
        "#Use BERT (any model that has SequenceClassification class from HuggingFace would work here)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Instantiate trainer that handles fitting.\n",
        "trainer = transformer_trainer.TransformerTrainer(model=model,\n",
        "  train_loader=train_loader,\n",
        "  val_loader=val_loader, test_loader=test_loader,\n",
        "  num_ns_eval=int(average_relevant_per_query), task_type=\"classification\", tokenizer=tokenizer,\n",
        "  validate_every_epochs=1, num_validation_instances=-1,\n",
        "  num_epochs=1, lr=5e-7, sacred_ex=None)\n",
        "\n",
        "#Train (our validation eval uses the NS sampling procedure)\n",
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y0fLLW7wV3YE"
      },
      "source": [
        "## Evaluating with ClariQ evaluation scripts\n",
        "The above code uses the transformer-ranker's built-in evaluation. This means that we are only ranking from a set of K (int(average_relevant_per_query) in the example) candidate questions, a re-ranking scenario where all the positive examples are in the candidate list. RQ2 of ClariQ requires us to rank from the entire question_bank. Additionally it evaluates whether the clarifying questions helps for document retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "colab_type": "code",
        "id": "t4BSJT7vVT01",
        "outputId": "a067d813-6093-44a0-fe1f-6a941984422f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ClariQ-repo'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Counting objects: 100% (108/108), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 120 (delta 56), reused 77 (delta 33), pack-reused 12\u001b[K\n",
            "Receiving objects: 100% (120/120), 24.07 MiB | 30.13 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.18.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/aliannejadi/ClariQ.git ClariQ-repo\n",
        "! pip install rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rguln-2txb5o"
      },
      "source": [
        "### Re-rank BM25 with the fine-tuned BERT-ranker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gnToD3YFAAXK"
      },
      "source": [
        "Lets first use the bm25 [example](https://colab.research.google.com/drive/1g_Sc9j5fYT1hiOxif6BVH5NHNt-icxtT#scrollTo=_7_2LTXoXqK7) from Mohammad to generate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6HgNLYQJ1PaC"
      },
      "outputs": [],
      "source": [
        "rerank_top_k = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "39S7b3jvAG5P",
        "outputId": "367c4037-c065-495d-d0e2-6eea330ddae0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Imports required packages, defines stem & tokenizez function\n",
        "import pandas as pd\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def stem_tokenize(text, remove_stopwords=True):\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text) \\\n",
        "                                      for word in nltk.word_tokenize(sent)]\n",
        "  tokens = [word for word in tokens if word not in \\\n",
        "          nltk.corpus.stopwords.words('english')]\n",
        "  return [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "# Files paths\n",
        "request_file_path = './ClariQ-repo/data/dev.tsv'\n",
        "question_bank_path = './ClariQ-repo/data/question_bank.tsv'\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_bm25'\n",
        "\n",
        "# Reads files and build bm25 corpus (index)\n",
        "dev = pd.read_csv(request_file_path, sep='\\t')\n",
        "question_bank = pd.read_csv(question_bank_path, sep='\\t').fillna('')\n",
        "question_bank['tokenized_question_list'] = question_bank['question'].map(stem_tokenize)\n",
        "question_bank['tokenized_question_str'] = question_bank['tokenized_question_list'].map(lambda x: ' '.join(x))\n",
        "bm25_corpus = question_bank['tokenized_question_list'].tolist()\n",
        "bm25 = BM25Okapi(bm25_corpus)\n",
        "\n",
        "# Runs bm25 for every query and stores output in file.\n",
        "examples = []\n",
        "all_preds_bm25 = []\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    bm25_ranked_list = bm25.get_top_n(stem_tokenize(query, True), \n",
        "                                    bm25_corpus, \n",
        "                                    n=rerank_top_k)\n",
        "    bm25_q_list = [' '.join(sent) for sent in bm25_ranked_list]\n",
        "    docs = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question'].tolist()\n",
        "    preds = question_bank.set_index('tokenized_question_str').loc[bm25_q_list, 'question_id'].tolist()\n",
        "    all_preds_bm25.append(preds)\n",
        "    for doc in docs[:rerank_top_k]:\n",
        "      examples.append((query, doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EELGiVqv6_6L"
      },
      "source": [
        "Now we are going to transform this dataset in the format required for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DHBszQljAT_i"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import DefaultDataCollator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformer_rankers.utils import utils\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)\n",
        "collator = DefaultDataCollator()\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collator.collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mXPpw7FTE2x2"
      },
      "source": [
        "Now we can run the trained model on this dataset and  save the predictions to a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "mVGhE-yQAvH8",
        "outputId": "a4f12be0-a01d-46b2-d9e3-49db114e15eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 94/94 [00:10<00:00,  9.37it/s]\n"
          ]
        }
      ],
      "source": [
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], rerank_top_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tVDAJuVEAwZk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-reranker'\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    document_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_k_scores_idx = (-document_scores).argsort()[:rerank_top_k]  \n",
        "    preds = np.array(all_preds_bm25[tid_idx])[top_k_scores_idx]\n",
        "    for i, qid in enumerate(preds):\n",
        "      fo.write('{} 0 {} {} {} BERT-reranker\\n'.format(tid, qid, i, len(preds)-i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "Dy9GH-rDCAyx",
        "outputId": "ddce776d-e41f-49db-cb83-f21abcc1f5f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall5: 0.3474806038474769\n",
            "Recall10: 0.6136149763549145\n",
            "Recall20: 0.6912818698329535\n",
            "Recall30: 0.6912818698329535\n"
          ]
        }
      ],
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "YEiMf2pqB0XC",
        "outputId": "ec6e89ac-a363-48c8-e43c-2ac5bcc9098b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG1: 0.18958333333333333\n",
            "NDCG3: 0.17431329825264302\n",
            "NDCG5: 0.16796281956102732\n",
            "NDCG10: 0.1658691210524936\n",
            "NDCG20: 0.1527795302714777\n",
            "P1: 0.2375\n",
            "P3: 0.20416666666666666\n",
            "P5: 0.19\n",
            "P10: 0.176875\n",
            "P20: 0.1384375\n",
            "MRR100: 0.33301824879980596\n"
          ]
        }
      ],
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uBFowAVmtQH4"
      },
      "source": [
        "### Full retrieval\n",
        "So let's first generate a dataset containing all combinations of dev queries \n",
        "and question_bank questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jB2VwOPlc1ol"
      },
      "outputs": [],
      "source": [
        "from transformers.data.data_collator import DefaultDataCollator\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, index):\n",
        "        return self.features[index]\n",
        "\n",
        "#Lets not use the null document for no question.\n",
        "all_documents = list(question_bank[\"question\"].values[1:])\n",
        "examples = []\n",
        "for tid in dev['topic_id'].unique():\n",
        "    query = dev.loc[dev['topic_id']==tid, 'initial_request'].tolist()[0]\n",
        "    for doc in all_documents:\n",
        "      examples.append((query, doc))\n",
        "\n",
        "batch_encoding = tokenizer.batch_encode_plus(examples, \n",
        "                max_length=max_seq_len, pad_to_max_length=True)\n",
        "features = []\n",
        "for i in range(len(examples)):\n",
        "    inputs = {k: batch_encoding[k][i] for k in batch_encoding}\n",
        "    feature = InputFeatures(**inputs, label=0)\n",
        "    features.append(feature)\n",
        "\n",
        "dataset = SimpleDataset(features)\n",
        "collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ddUbNbzGt73A"
      },
      "source": [
        "Now we have to make the predictions and acumulate the logits by the number of candidate documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "SLF1qpmRpBZL",
        "outputId": "98355783-92e9-4fb4-dc66-ec3a0d7d734c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12313/12313 [21:48<00:00,  9.41it/s]\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collator.collate_batch)\n",
        "from transformer_rankers.utils import utils\n",
        "logits, _, softmax_output = trainer.predict(dataloader)\n",
        "softmax_output_by_query = utils.acumulate_list(softmax_output[0], len(all_documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QefshgrStfja"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "run_file_path = './ClariQ-repo/sample_runs/dev_BERT-ranker'\n",
        "all_doc_ids = np.array(question_bank[\"question_id\"].values[1:])\n",
        "with open(run_file_path, 'w') as fo:\n",
        "  for tid_idx, tid in enumerate(dev['topic_id'].unique()):\n",
        "    all_documents_scores = np.array(softmax_output_by_query[tid_idx])\n",
        "    top_30_scores_idx = (-all_documents_scores).argsort()[:30]  \n",
        "    preds = all_doc_ids[top_30_scores_idx]\n",
        "    for i, qid in enumerate(preds):    \n",
        "      fo.write('{} 0 {} {} {} BERT-ranker\\n'.format(tid, qid, i, len(preds)-i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "8NlCqE7XwSh-",
        "outputId": "697a5b5f-e175-4d2b-f168-645186abacc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall5: 0.35055278656671846\n",
            "Recall10: 0.6154512724117988\n",
            "Recall20: 0.7253078340648\n",
            "Recall30: 0.7529370626793227\n"
          ]
        }
      ],
      "source": [
        "# Report question relevance performance\n",
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task question_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}_question_relevance.eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "1Znqwz6ZwUXY",
        "outputId": "d302545a-6c0f-4fc3-84e2-e165b5afe5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG1: 0.18958333333333333\n",
            "NDCG3: 0.17431329825264302\n",
            "NDCG5: 0.16796281956102732\n",
            "NDCG10: 0.1658691210524936\n",
            "NDCG20: 0.1527795302714777\n",
            "P1: 0.2375\n",
            "P3: 0.20416666666666666\n",
            "P5: 0.19\n",
            "P10: 0.176875\n",
            "P20: 0.1384375\n",
            "MRR100: 0.33301824879980596\n"
          ]
        }
      ],
      "source": [
        "! python ./ClariQ-repo/src/clariq_eval_tool.py  --eval_task document_relevance\\\n",
        "                                                --data_dir ./ClariQ-repo/data/ \\\n",
        "                                                --experiment_type dev \\\n",
        "                                                --run_file {run_file_path} \\\n",
        "                                                --out_file {run_file_path}.eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UKArX9JZBRUI"
      },
      "source": [
        "## Results comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "sDPJh2oX_i4G",
        "outputId": "24aeeafd-83ff-4a13-e2f3-91b4e6f2721a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>Recall5</th>\n",
              "      <th>Recall10</th>\n",
              "      <th>Recall20</th>\n",
              "      <th>Recall30</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.3246</td>\n",
              "      <td>0.5638</td>\n",
              "      <td>0.6675</td>\n",
              "      <td>0.6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-reranker</th>\n",
              "      <td>0.3475</td>\n",
              "      <td>0.6136</td>\n",
              "      <td>0.6913</td>\n",
              "      <td>0.6913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-ranker</th>\n",
              "      <td>0.3506</td>\n",
              "      <td>0.6155</td>\n",
              "      <td>0.7253</td>\n",
              "      <td>0.7529</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                value                           \n",
              "metric        Recall5 Recall10 Recall20 Recall30\n",
              "model                                           \n",
              "bm25           0.3246   0.5638   0.6675   0.6913\n",
              "BERT-reranker  0.3475   0.6136   0.6913   0.6913\n",
              "BERT-ranker    0.3506   0.6155   0.7253   0.7529"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}_question_relevance.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "cols = res_df.columns.tolist()\n",
        "res_df.sort_values([(\"value\",\"Recall10\")])[cols[-1:] + cols[:-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "colab_type": "code",
        "id": "e4SF6f7RDptU",
        "outputId": "6029df47-4213-46ba-b1bc-cff38ed49909"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"11\" halign=\"left\">value</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>metric</th>\n",
              "      <th>MRR100</th>\n",
              "      <th>NDCG1</th>\n",
              "      <th>NDCG10</th>\n",
              "      <th>NDCG20</th>\n",
              "      <th>NDCG3</th>\n",
              "      <th>NDCG5</th>\n",
              "      <th>P1</th>\n",
              "      <th>P10</th>\n",
              "      <th>P20</th>\n",
              "      <th>P3</th>\n",
              "      <th>P5</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>bm25</th>\n",
              "      <td>0.3096</td>\n",
              "      <td>0.1859</td>\n",
              "      <td>0.1363</td>\n",
              "      <td>0.1285</td>\n",
              "      <td>0.1608</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.2313</td>\n",
              "      <td>0.1406</td>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-ranker</th>\n",
              "      <td>0.3330</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.1659</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.1743</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.1384</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BERT-reranker</th>\n",
              "      <td>0.3330</td>\n",
              "      <td>0.1896</td>\n",
              "      <td>0.1659</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.1743</td>\n",
              "      <td>0.168</td>\n",
              "      <td>0.2375</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.1384</td>\n",
              "      <td>0.2042</td>\n",
              "      <td>0.190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                value                          ...                               \n",
              "metric         MRR100   NDCG1  NDCG10  NDCG20  ...     P10     P20      P3     P5\n",
              "model                                          ...                               \n",
              "bm25           0.3096  0.1859  0.1363  0.1285  ...  0.1406  0.1181  0.1896  0.175\n",
              "BERT-ranker    0.3330  0.1896  0.1659  0.1528  ...  0.1769  0.1384  0.2042  0.190\n",
              "BERT-reranker  0.3330  0.1896  0.1659  0.1528  ...  0.1769  0.1384  0.2042  0.190\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "models = [\"bm25\", \"BERT-reranker\", \"BERT-ranker\"]\n",
        "results = []\n",
        "for model in models:\n",
        "  with open('./ClariQ-repo/sample_runs/dev_{}.eval'.format(model)) as f:\n",
        "    res = json.load(f)\n",
        "    for metric_name in res:\n",
        "        metric_avg = np.mean([res[metric_name][k] for k in res[metric_name]])\n",
        "        results.append([model, metric_name, metric_avg])\n",
        "res_df = pd.DataFrame(results, columns = [\"model\", \"metric\", \"value\"])\n",
        "\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "res_df = res_df.set_index([\"model\", \"metric\"]).unstack()\n",
        "res_df.sort_values((\"value\", \"MRR100\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6nlZ6KRfCS-n"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT-ranker_for_ClariQ.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02932572abae45d0b615af4c68af473e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a3f5ed58d9d4d308643184b11cef78a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "131daad9d4bb457c84510b5326ed4cf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d48517fab1f4826a99b5053f71ccf5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e66069b612964f37b897276d35fb341b",
              "IPY_MODEL_5e0c3ce7cd944b92afc1ec5fd5c21773"
            ],
            "layout": "IPY_MODEL_cd8edeb69bca4a6aa6aa644cd3c896d6"
          }
        },
        "251e3d3acb5b49e7ba596286f5b367ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2996a43d436a4d7aae36c7d44626af4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5690414a8bd544aca30f2fd97a3cf905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0c3ce7cd944b92afc1ec5fd5c21773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2996a43d436a4d7aae36c7d44626af4a",
            "placeholder": "​",
            "style": "IPY_MODEL_0a3f5ed58d9d4d308643184b11cef78a",
            "value": " 232k/232k [00:00&lt;00:00, 1.24MB/s]"
          }
        },
        "921b8a33cba14558b84c9c1916e47e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_979b0281fd4e49df865b0c184e8d1bb6",
              "IPY_MODEL_f21f758f54e8481c973853989e87d200"
            ],
            "layout": "IPY_MODEL_5690414a8bd544aca30f2fd97a3cf905"
          }
        },
        "94d38c0feeff4b768ab3188ec4440f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131daad9d4bb457c84510b5326ed4cf5",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1fe00e2fd6f4d36b844ec1bdc39d6a1",
            "value": 440473133
          }
        },
        "979b0281fd4e49df865b0c184e8d1bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_251e3d3acb5b49e7ba596286f5b367ab",
            "max": 433,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ade0cde134a84b63bedde4facd44f428",
            "value": 433
          }
        },
        "9d07b3f2a9094d64867f893667c253bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f88dcb06d364fcb9be11a7980b10775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "a05fb8ec4b844c69b0d8adc1b659168f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7653c51b7b44cb783278ee3fd4f5d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5ed1b54d7ef464c831572fc2ef22523",
            "placeholder": "​",
            "style": "IPY_MODEL_e289eaabf4964d50824ed92df65b9744",
            "value": " 440M/440M [08:37&lt;00:00, 851kB/s]"
          }
        },
        "ade0cde134a84b63bedde4facd44f428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "bc067504beca4713b3a5796551a99414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94d38c0feeff4b768ab3188ec4440f35",
              "IPY_MODEL_a7653c51b7b44cb783278ee3fd4f5d59"
            ],
            "layout": "IPY_MODEL_02932572abae45d0b615af4c68af473e"
          }
        },
        "c1fe00e2fd6f4d36b844ec1bdc39d6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "cd8edeb69bca4a6aa6aa644cd3c896d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db8f058e9381429686f007982fcc577d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e289eaabf4964d50824ed92df65b9744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5ed1b54d7ef464c831572fc2ef22523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e66069b612964f37b897276d35fb341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Downloading: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d07b3f2a9094d64867f893667c253bf",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f88dcb06d364fcb9be11a7980b10775",
            "value": 231508
          }
        },
        "f21f758f54e8481c973853989e87d200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05fb8ec4b844c69b0d8adc1b659168f",
            "placeholder": "​",
            "style": "IPY_MODEL_db8f058e9381429686f007982fcc577d",
            "value": " 433/433 [00:00&lt;00:00, 1.15kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
